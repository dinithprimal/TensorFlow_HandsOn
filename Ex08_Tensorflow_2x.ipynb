{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPHr7G+xoWByttt2h1cyBoT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dinithprimal/TensorFlow_HandsOn/blob/main/Ex08_Tensorflow_2x.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing\n",
        "\n",
        "## RNN Poem/Play Generator"
      ],
      "metadata": {
        "id": "O9G8qVv6mw2R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNNubb4enCZF",
        "outputId": "7ec8ca73-3c8a-4eaa-bca7-e61d86f64367"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset"
      ],
      "metadata": {
        "id": "mURDG2t75a7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "taILMgDp5iLB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c368027-0c9f-48b4-ca1b-95d98f3fcb7f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading Data"
      ],
      "metadata": {
        "id": "lQpOs6gd6AuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#path_to_file = list(files.upload().keys())[0]"
      ],
      "metadata": {
        "id": "ibEfoeWg6EgI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read Contents of File"
      ],
      "metadata": {
        "id": "YFxlAEwx6Pwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# Length of text is the number of characters in it\n",
        "print('Length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5m0mVuV6l3a",
        "outputId": "d7198c14-b6a7-4ad1-9820-aceaff2dca79"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7zMrLV96_6p",
        "outputId": "babfc669-407e-45aa-e942-61c38ca161d6"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding"
      ],
      "metadata": {
        "id": "lIxIGkhv7Hql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "\n",
        "# Creating a mapping from unique characters to indices\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "RetJsXD-7NGn"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at how part of our text is encoded\n",
        "print(\"Text:\", text[:13])\n",
        "print(\"Encoded:\", text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvw9LQIV8OJq",
        "outputId": "e071bcbd-a06f-4af6-c9a8-a108a1ead280"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# And here we will make a function that can convert our numeric values to text\n",
        "\n",
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w97XSVuN8h3l",
        "outputId": "fe60187f-d458-433e-f8cf-c83209f37165"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Training Examples"
      ],
      "metadata": {
        "id": "6dE8Bs3A-AYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Our first step will be to create a stream of characters from out text data\n",
        "\n",
        "seq_length = 100 # length of sequence for a training example\n",
        "example_per_epoch = len(text)//(seq_length + 1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)"
      ],
      "metadata": {
        "id": "Gc1iwy4X-FW8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next we can use the batch method to turn this stream of characters into batches of desired lenght\n",
        "\n",
        "sequences = char_dataset.batch(seq_length + 1, drop_remainder=True)"
      ],
      "metadata": {
        "id": "rd2_UPZi-ybP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we need to use these sequences of length 101 and split them into input and output\n",
        "\n",
        "def split_input_target(chunk):  # for the example: hello\n",
        "  input_text = chunk[:-1]  # hell\n",
        "  target_text = chunk[1:]  # ello\n",
        "  return input_text, target_text  # hell, ello\n",
        "\n",
        "dataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry"
      ],
      "metadata": {
        "id": "xkQ4SW_P_YU7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print('\\n\\nEXAMPLE\\n')\n",
        "  print(\"INPUT\")\n",
        "  print(int_to_text(x))\n",
        "  print(\"\\nOUTPUT\")\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SR24MLyANK2",
        "outputId": "ced9b278-9459-409b-ce14-9e1b96135345"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUTPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUTPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally we need to make teaining batches\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab) # vocab is number of unique characters\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite squence,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "JwyhYLPvCG8K"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the Model"
      ],
      "metadata": {
        "id": "oZTu6WmODmY6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Embedding(\n",
        "          vocab_size,\n",
        "          embedding_dim,\n",
        "          batch_input_shape=[batch_size, None]\n",
        "      ),\n",
        "      tf.keras.layers.LSTM(\n",
        "          rnn_units,\n",
        "          return_sequences=True,\n",
        "          stateful=True,\n",
        "          recurrent_initializer='glorot_uniform'\n",
        "      ),\n",
        "      tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93AoLERVDrZD",
        "outputId": "df4dd650-3279-44c4-a88f-192c4c0a6afc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5330241 (20.33 MB)\n",
            "Trainable params: 5330241 (20.33 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Loss Function"
      ],
      "metadata": {
        "id": "8cFKP91wHlq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch) # ask our model for a prediction on out first batch of training data\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\") # print out the shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2lZkdR3Hv8B",
        "outputId": "ca3706c4-15e8-4adf-ce2c-8154d1c2db3e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we can see that the prediction is an array of 64 arrays, one for each entry in the batch\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LeqTGBiKfon",
        "outputId": "3d7f16b1-ce1b-4ee5-a7ce-2dfcdb529f54"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[ 2.1278937e-03  3.0047502e-03  4.5565674e-03 ...  1.4587980e-03\n",
            "   -7.2790956e-04 -4.3709916e-03]\n",
            "  [ 6.4861807e-03 -2.2537396e-03  3.6658992e-03 ...  3.7494134e-03\n",
            "    1.0263856e-04 -2.8702314e-03]\n",
            "  [ 8.8444427e-03 -2.2567031e-03  6.9050938e-03 ... -3.5881503e-03\n",
            "   -6.1919284e-03  3.2912369e-03]\n",
            "  ...\n",
            "  [ 3.6886791e-03 -1.1928793e-02  4.4608433e-03 ...  1.4310230e-03\n",
            "    1.0113213e-03  6.6531096e-03]\n",
            "  [ 3.5818722e-03 -9.0815742e-03  1.3127537e-03 ... -4.6453113e-03\n",
            "    3.3510458e-03  6.5625142e-03]\n",
            "  [ 1.0883348e-03 -7.8963786e-03 -6.2328391e-03 ... -3.8814968e-03\n",
            "    5.7682223e-03  5.3348620e-03]]\n",
            "\n",
            " [[-4.7675902e-03 -4.7176732e-03  1.5749282e-03 ...  3.2389574e-03\n",
            "    3.6611832e-03 -6.0999068e-03]\n",
            "  [ 4.5263121e-05 -6.5601366e-03  5.8718710e-03 ...  3.0717528e-03\n",
            "    3.9296523e-03 -4.4152783e-03]\n",
            "  [ 5.1376470e-03 -5.3713545e-03  7.6626013e-03 ... -2.4569300e-03\n",
            "   -1.3814103e-03  2.3111580e-03]\n",
            "  ...\n",
            "  [-1.5195100e-02  2.8340169e-04 -1.1025434e-02 ...  8.7945694e-03\n",
            "    1.8729037e-03 -1.0822888e-02]\n",
            "  [-1.2009599e-02 -4.8272228e-03 -1.7147915e-02 ...  6.4685652e-03\n",
            "    4.9421778e-03 -7.5723771e-03]\n",
            "  [-8.8822935e-03  5.1753968e-04 -1.9023811e-02 ...  8.8423323e-03\n",
            "   -1.8294421e-03 -6.3520782e-03]]\n",
            "\n",
            " [[-5.7880157e-03  5.3846827e-03 -4.2927838e-03 ... -4.9745291e-04\n",
            "   -3.6299089e-03 -5.0198818e-03]\n",
            "  [-9.0686260e-03 -7.8256242e-04 -1.4274538e-03 ...  3.5765837e-03\n",
            "    1.7487217e-03 -1.0471623e-02]\n",
            "  [-8.4855603e-03  1.5533760e-03 -4.4856528e-03 ...  1.5969735e-03\n",
            "    2.5242832e-03 -7.0752371e-03]\n",
            "  ...\n",
            "  [ 7.3750825e-03 -1.3077507e-02  4.4038091e-03 ...  2.6027928e-03\n",
            "    5.3259665e-03  2.5080233e-03]\n",
            "  [ 3.1676376e-04 -1.6235678e-02  4.1950867e-03 ...  4.3254495e-03\n",
            "    8.4625129e-03 -3.9599175e-03]\n",
            "  [ 7.3638186e-04 -1.3788406e-02  4.7977380e-03 ...  8.1584621e-03\n",
            "    5.8128759e-03 -6.1685825e-04]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1.4628033e-03  2.4300183e-03 -7.3013321e-04 ... -9.0412458e-04\n",
            "   -8.5954653e-04  9.1346670e-03]\n",
            "  [ 4.6541090e-03 -1.2339787e-03  4.8849527e-03 ... -1.0979234e-04\n",
            "   -7.0094242e-04  7.5005027e-03]\n",
            "  [-7.6541997e-04 -6.2084263e-03  5.2238256e-03 ...  3.3545610e-03\n",
            "    3.6041648e-03 -3.2157334e-04]\n",
            "  ...\n",
            "  [-4.2606783e-03 -4.0096068e-03 -9.3293982e-03 ... -8.3930779e-04\n",
            "   -3.0810728e-03  5.0293687e-03]\n",
            "  [-2.8491104e-03  9.8655873e-04 -1.3644554e-02 ...  3.4131180e-03\n",
            "   -9.0040546e-03  4.5739347e-03]\n",
            "  [-5.3593400e-03  4.9971754e-04 -7.3061902e-03 ...  2.1589948e-03\n",
            "    4.3160288e-04  3.6726033e-03]]\n",
            "\n",
            " [[ 1.1510409e-03  1.2611330e-03  4.9268566e-03 ... -8.2398411e-03\n",
            "   -5.6831772e-03  2.5935946e-03]\n",
            "  [ 4.1610552e-03 -1.7790456e-03  8.7454319e-03 ... -6.1230836e-03\n",
            "   -3.8897716e-03  2.6444003e-03]\n",
            "  [ 1.8950907e-03 -2.7271186e-03 -9.4456878e-04 ... -5.9442432e-03\n",
            "    5.3106633e-06  1.8189882e-03]\n",
            "  ...\n",
            "  [ 1.5238610e-03  5.1601664e-03 -2.7254201e-03 ...  2.8429434e-03\n",
            "   -1.4394539e-02 -9.7900033e-03]\n",
            "  [ 7.3274598e-04 -1.4712790e-03 -9.5635829e-03 ... -1.1391693e-03\n",
            "   -8.3812140e-03 -6.2924162e-03]\n",
            "  [ 1.0132026e-03  1.2635071e-03 -7.8936256e-03 ... -2.2420750e-03\n",
            "   -5.4536518e-03  3.3380326e-03]]\n",
            "\n",
            " [[ 1.4628033e-03  2.4300183e-03 -7.3013321e-04 ... -9.0412458e-04\n",
            "   -8.5954653e-04  9.1346670e-03]\n",
            "  [ 2.9741228e-03  5.2111214e-03 -5.9956629e-03 ...  2.2645351e-03\n",
            "   -8.9989826e-03  7.2084130e-03]\n",
            "  [ 1.6681990e-04  3.1969948e-03 -7.4205355e-04 ...  3.0496984e-04\n",
            "   -8.0313301e-04  5.1092422e-03]\n",
            "  ...\n",
            "  [ 1.3119825e-03 -5.0718598e-03  3.0744094e-03 ...  3.0028636e-03\n",
            "    1.6039374e-03 -1.1752457e-03]\n",
            "  [ 5.2149920e-04 -1.8617024e-03  5.1022782e-03 ... -2.7045142e-03\n",
            "    1.1738427e-03 -1.2085461e-03]\n",
            "  [ 1.0034442e-04  1.5512914e-03  4.0488453e-03 ... -5.9301211e-03\n",
            "    1.6025505e-03 -3.4746970e-04]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's examine one prediction\n",
        "pred = example_batch_predictions[0]\n",
        "print(len(pred))\n",
        "print(pred)\n",
        "# notice this is a 2D array of length 100, where each interior array is the prediction for the next character at each time step"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZOgz3O-Kxdy",
        "outputId": "ea563be2-734f-49a0-8e81-72ec24813fd3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[ 0.00212789  0.00300475  0.00455657 ...  0.0014588  -0.00072791\n",
            "  -0.00437099]\n",
            " [ 0.00648618 -0.00225374  0.0036659  ...  0.00374941  0.00010264\n",
            "  -0.00287023]\n",
            " [ 0.00884444 -0.0022567   0.00690509 ... -0.00358815 -0.00619193\n",
            "   0.00329124]\n",
            " ...\n",
            " [ 0.00368868 -0.01192879  0.00446084 ...  0.00143102  0.00101132\n",
            "   0.00665311]\n",
            " [ 0.00358187 -0.00908157  0.00131275 ... -0.00464531  0.00335105\n",
            "   0.00656251]\n",
            " [ 0.00108833 -0.00789638 -0.00623284 ... -0.0038815   0.00576822\n",
            "   0.00533486]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# and finally well look at a prediction at the first timestep\n",
        "time_pred = pred[0]\n",
        "print(len(time_pred))\n",
        "print(time_pred)\n",
        "# and of course its 65 values representing the probability of each character accuring next"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cnqN5-TDLsu_",
        "outputId": "14b7b052-2708-4cff-dbdb-2377940f6c8f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65\n",
            "tf.Tensor(\n",
            "[ 0.00212789  0.00300475  0.00455657 -0.00336457 -0.00432321  0.00034721\n",
            "  0.00118927 -0.00035017 -0.00702676 -0.00068105 -0.00453394 -0.00549321\n",
            " -0.00454005 -0.00347712 -0.00677659 -0.0017342   0.00234554  0.003143\n",
            "  0.00266618  0.00218665  0.00045251 -0.00227905 -0.00145212 -0.00411965\n",
            " -0.00163225 -0.00215978  0.00505199  0.00265671 -0.00137684  0.00215418\n",
            " -0.00330329 -0.00345043 -0.0008038   0.00539538 -0.00147806 -0.00790671\n",
            " -0.00685127 -0.00170341 -0.00140664  0.00284512 -0.0014504  -0.00050722\n",
            "  0.00086997 -0.00538255 -0.00225143  0.00241805  0.00496998 -0.00509916\n",
            "  0.00518108 -0.00023781 -0.00053365  0.00109739  0.00341649  0.00294521\n",
            "  0.00426318  0.00426968  0.00964413 -0.00349653  0.00483308 -0.00270794\n",
            "  0.00266954  0.00148114  0.0014588  -0.00072791 -0.00437099], shape=(65,), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# If we want to determine the predicted character we need to samle the output distribution (pick a value based on probability)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "# now we can reshape that array and convert all tha integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sampled_indices, (1, -1))[0]\n",
        "predicted_chars = int_to_text(sampled_indices)\n",
        "\n",
        "predicted_chars  # and this is what the model predicted for training sequence 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "QEMtT1hrMG3u",
        "outputId": "302bbd91-0d07-4b3d-dbe5-58149eae45b4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"azkZ'p?,JpJ'BKcBrl-odh?p-YS;pFK3?Sx.IFqbNlwFofzQKamDjszFBXQc3ICapxD n!&hKN:\\nqAxObHUCtpWCiKwez.x3TyRN\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ],
      "metadata": {
        "id": "rHb9cp6bNe-H"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compiling the Model"
      ],
      "metadata": {
        "id": "8AyKSQHrPfX7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "j33JPJMlPiNL"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Checkpoints"
      ],
      "metadata": {
        "id": "LH-YB-84P6nL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directinal where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True\n",
        ")"
      ],
      "metadata": {
        "id": "14TSsMySQJDx"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "QFAwS0D2Qve_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs=40, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5dXxFraQyR_",
        "outputId": "c0079ae7-331d-44be-c601-9683b6f423f3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "172/172 [==============================] - 20s 74ms/step - loss: 2.5744\n",
            "Epoch 2/40\n",
            "172/172 [==============================] - 13s 66ms/step - loss: 1.9003\n",
            "Epoch 3/40\n",
            "172/172 [==============================] - 13s 68ms/step - loss: 1.6498\n",
            "Epoch 4/40\n",
            "172/172 [==============================] - 14s 69ms/step - loss: 1.5123\n",
            "Epoch 5/40\n",
            "172/172 [==============================] - 14s 71ms/step - loss: 1.4290\n",
            "Epoch 6/40\n",
            "172/172 [==============================] - 14s 74ms/step - loss: 1.3720\n",
            "Epoch 7/40\n",
            "172/172 [==============================] - 15s 79ms/step - loss: 1.3267\n",
            "Epoch 8/40\n",
            "172/172 [==============================] - 17s 81ms/step - loss: 1.2868\n",
            "Epoch 9/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 1.2508\n",
            "Epoch 10/40\n",
            "172/172 [==============================] - 14s 75ms/step - loss: 1.2162\n",
            "Epoch 11/40\n",
            "172/172 [==============================] - 15s 74ms/step - loss: 1.1814\n",
            "Epoch 12/40\n",
            "172/172 [==============================] - 15s 79ms/step - loss: 1.1465\n",
            "Epoch 13/40\n",
            "172/172 [==============================] - 16s 82ms/step - loss: 1.1102\n",
            "Epoch 14/40\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 1.0726\n",
            "Epoch 15/40\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 1.0336\n",
            "Epoch 16/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.9936\n",
            "Epoch 17/40\n",
            "172/172 [==============================] - 16s 81ms/step - loss: 0.9540\n",
            "Epoch 18/40\n",
            "172/172 [==============================] - 15s 78ms/step - loss: 0.9145\n",
            "Epoch 19/40\n",
            "172/172 [==============================] - 14s 73ms/step - loss: 0.8744\n",
            "Epoch 20/40\n",
            "172/172 [==============================] - 15s 78ms/step - loss: 0.8377\n",
            "Epoch 21/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.7999\n",
            "Epoch 22/40\n",
            "172/172 [==============================] - 15s 78ms/step - loss: 0.7660\n",
            "Epoch 23/40\n",
            "172/172 [==============================] - 16s 78ms/step - loss: 0.7345\n",
            "Epoch 24/40\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.7047\n",
            "Epoch 25/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.6775\n",
            "Epoch 26/40\n",
            "172/172 [==============================] - 16s 81ms/step - loss: 0.6525\n",
            "Epoch 27/40\n",
            "172/172 [==============================] - 16s 76ms/step - loss: 0.6302\n",
            "Epoch 28/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.6104\n",
            "Epoch 29/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.5911\n",
            "Epoch 30/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.5742\n",
            "Epoch 31/40\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.5589\n",
            "Epoch 32/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.5439\n",
            "Epoch 33/40\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.5308\n",
            "Epoch 34/40\n",
            "172/172 [==============================] - 15s 80ms/step - loss: 0.5206\n",
            "Epoch 35/40\n",
            "172/172 [==============================] - 15s 76ms/step - loss: 0.5110\n",
            "Epoch 36/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.5023\n",
            "Epoch 37/40\n",
            "172/172 [==============================] - 15s 80ms/step - loss: 0.4943\n",
            "Epoch 38/40\n",
            "172/172 [==============================] - 15s 78ms/step - loss: 0.4846\n",
            "Epoch 39/40\n",
            "172/172 [==============================] - 15s 75ms/step - loss: 0.4779\n",
            "Epoch 40/40\n",
            "172/172 [==============================] - 15s 77ms/step - loss: 0.4713\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Model"
      ],
      "metadata": {
        "id": "5vMYhB8qRAj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "I43-8kDUR3lg"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# once the model is finished training we can find the latest checkpoint that stores the models weights using the following line\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "GIbJ73hbUA2W"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can load any checkpoint we want by specifying the exact file to load\n",
        "\n",
        "# checkpoint_num = 10\n",
        "# model.load_weights(tf.train.load_checkpoint('./training_checkpoints/ckpt_' + str(checkpoint_num)))\n",
        "# model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "yBFyg7BGUgsm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating text"
      ],
      "metadata": {
        "id": "_5cUJZJjVLA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generated_text(model, start_string):\n",
        "  # Evaluate step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 800\n",
        "\n",
        "  # converting our start string to numbers (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our result\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperature results in more predictable text.\n",
        "  # Higher temperature results in more surprising text\n",
        "  # Experment to find the best setting\n",
        "  temperature = 1.0\n",
        "\n",
        "  # here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    # Remove the batch dimension\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "    # Using a categorical distribution to predict the character returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "    # we pass the predicted character as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "uipQk40nVsu7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input(\"Type a starting string: \")\n",
        "print(generated_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTohKXeUYHlp",
        "outputId": "ec6c65d5-5036-42af-891f-4e0d186f6a68"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: Romeo \n",
            "Romeo pocket! his haste, and hast the time to prison.\n",
            "\n",
            "GLOUCESTER:\n",
            "Alas, that valiant you wot your faith in Paul,\n",
            "Have no py sll pluck him home, that he hath too much of\n",
            "you take our steed; know you what! by your father's voices?\n",
            "\n",
            "MENENIUS:\n",
            "Note me that made thyself, but saduted\n",
            "With thy embracements, but suspecting an her sister'd from the seas.\n",
            "\n",
            "KING RICHARD III:\n",
            "Madam, how now! What! what! I am gower when I am, not\n",
            "Richard, by Bolingbroke say I'll be you think if\n",
            "He had some husband send to be of king.\n",
            "\n",
            "GLOUCESTER:\n",
            "Yes, tarry, are still combeth it for your pleasure; having no\n",
            "ANGELOUSt the devil's dam:\n",
            "And so, my gentle queen, parters, joy overture,\n",
            "Which to the helps on the Tower and welcome.\n",
            "\n",
            "MIRANDA:\n",
            "How ble'd this knife be city,\n",
            "To lick their heart to do it said 'Ay.'\n",
            "And after tomp; one \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B6v4CoV9YYC_"
      },
      "execution_count": 28,
      "outputs": []
    }
  ]
}